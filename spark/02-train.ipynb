{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "#import pandas as pd\n",
    "import pyspark\n",
    "import os\n",
    "import sys\n",
    "import mlrun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer \n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from mlrun import get_or_create_ctx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mlrun] 2020-01-02 19:27:32,460 training started\n"
     ]
    }
   ],
   "source": [
    "# initialize context\n",
    "from mlrun import mlconf\n",
    "mlconf.dbpath = 'http://mlrun-api:8080'\n",
    "\n",
    "context = get_or_create_ctx('iris-train')\n",
    "\n",
    "context.logger.info(\"training started\")\n",
    "\n",
    "# start Spark session\n",
    "spark = pyspark.sql.SparkSession.builder.appName('Iris').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mlrun] 2020-01-02 19:27:50,291 Regularization Rate 0.01\n",
      "[mlrun] 2020-01-02 19:28:05,427 Accuracy 0.9803921568627451\n"
     ]
    }
   ],
   "source": [
    "#train_data = context.get_param('train_data',\"/User/mlrun_demos/spark/data/iris.csv\")\n",
    "train_data = context.get_param('train_data',\"v3io://users/admin/mlrun_demos/spark/data/iris.csv\")\n",
    "regularization_rate = context.get_param('regularization_rate',0.01)\n",
    "\n",
    "\n",
    "# load iris.csv into Spark dataframe\n",
    "schema = StructType([\n",
    "StructField(\"sepal-length\", FloatType(), True),\n",
    "StructField(\"sepal-width\", FloatType(), True),\n",
    "StructField(\"petal-length\", FloatType(), True),\n",
    "StructField(\"petal-width\", FloatType(), True),\n",
    "StructField(\"class\", StringType(), True)\n",
    "])\n",
    "data = spark.read.csv(train_data,header='false',schema=schema)\n",
    "\n",
    "# vectorize all numerical columns into a single feature column\n",
    "feature_cols = data.columns[:-1]\n",
    "assembler = VectorAssembler(inputCols=feature_cols,outputCol='features')\n",
    "data = assembler.transform(data)\n",
    "\n",
    "# convert text labels into indices\n",
    "data = data.select(['features', 'class'])\n",
    "label_indexer = StringIndexer(inputCol='class', outputCol='label').fit(data)\n",
    "data = label_indexer.transform(data)\n",
    "\n",
    "# only select the features and label column\n",
    "data = data.select(['features', 'label'])\n",
    "\n",
    "# log regularization rate\n",
    "context.logger.info(\"Regularization Rate \" + str(regularization_rate))\n",
    "\n",
    "# use Logistic Regression to train on the training set\n",
    "train, test = data.randomSplit([0.70, 0.30])\n",
    "lr = LogisticRegression(regParam=regularization_rate)\n",
    "model = lr.fit(train)\n",
    "\n",
    "# predict on the test set\n",
    "prediction = model.transform(test)\n",
    "#context.log_result(\"Prediction\",prediction.show(10))\n",
    "\n",
    "# evaluate the accuracy of the model using the test set\n",
    "evaluator = MulticlassClassificationEvaluator(metricName='accuracy')\n",
    "accuracy = evaluator.evaluate(prediction)\n",
    "\n",
    "# log accuracy\n",
    "context.logger.info('Accuracy ' + str(accuracy))\n",
    "context.log_result('accuracy',accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lr.write().overwrite().save('v3io://users/admin/tmp/model.model')\n",
    "model.write().overwrite().save('v3io://users/admin/tmp/model.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "context.log_artifact('model', src_path='/v3io/users/admin/tmp/model.model', labels={'framework': 'LogisticRegression'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
